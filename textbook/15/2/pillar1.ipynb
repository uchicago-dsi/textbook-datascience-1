{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pillar 1:  Data Transparency & Accountability\n",
    "\n",
    "**The attainment, usage, storage, analysis, and maintenance of human data should be as transparent, accountable, and honest as possible and intended for some sort of human benefit.**\n",
    "\n",
    "In order to facilitate reproducibility, trusted participation, and complete comprehension of terminal goals and results, a policy of transparency and honesty should be embedded into the protocol of handling human data. Transparency and honesty as it pertains to acquiring, using, storing, and analyzing data can include sharing information about these methods not only with stakeholders, but relevant government and regulatory agencies and the people that the data is collected from, when possible. Use of transparent and honest practices in each of these areas may increase the likelihood that the use of the data will in fact be beneficial to humans and may include, but are not limited to, the following practices:\n",
    "\n",
    "- Attainment - data was gathered through ethical and honest means, with the consent of subjects (See Pillar 2)\n",
    "- Usage - the use of data coincides with the intended use communicated to the subjects and does not include  practices that maliciously targets groups of people or spreads misinformation (See Pillar 4).\n",
    "- Analysis - proper analytical methods are applied to garner understanding of data and does not include biased or inappropriate statistical applications, such as p-hacking, cherry picking, selective omission, and other problematic research practices[^*]. This also includes conducting a meaningful cost-benefit analysis pertaining to accuracy vs. transparency.\n",
    "- Storage & Maintenance - secure means are used to preserve and house data physically or on an online server to ensure the privacy of subjects. Prior to collection, subjects are informed about what these privacy and security measures are (See Pillar 2).\n",
    "\n",
    "\n",
    "## Trust & Accountability\n",
    "\n",
    "\n",
    "The aforementioned practices may help to facilitate trust between human subjects/users and technologies that rely on data acquired from them [^**], [^***]. The establishment of trust within communities can have several advantages, including increased use of a product or technology, expanded collection of data, improved productivity and efficiency in tasks, and other benefits [^****]. However, with the occurence of numerous malevolent incidences, including massive data breaches that revealed sensitive financial information [^*****] or the use of \"synthetic\" media that has contributed to social and political confusion and misinformation [^******], it is unsurprising that some of the population has a level of distrust and concern when it comes to collection of personal data \n",
    "[^*******] and emergent technologies [^********] such as artificial intelligence (AI), machine learning (ML), and natural language processing (NLP), in the public and private sectors.\n",
    "\n",
    "\n",
    "\n",
    "In 2015, the Harvard Business Review published an article surveying consumers across several countries on sentiments around data privacy and security[^**********]. Interestingly, the entities people trusted most with their data were healthcare providers and financial institutions, while the least trusted were entertainment companies and social media firms. Arguably, the level of trust may coincide with mechanisms of accountability, such as HIPPA legislations and financial privacy laws like the Gramm–Leach–Bliley Act, which may not apply to entertainment and social media platforms in the same way. While legal obligation is a strong way to enforce accountability, providing open and accessible information on conduct around data security, privacy, and collection and protocols for when someone has questions or ailments about them shows an organization’s intrinsic commitment to transparent practices, which can enhance trust between the organization and customers, users, and other subjects from which data may be collected. \n",
    "\n",
    "<img align=\"center\" src=\"./img/trust.png\" width=\"75%\"/>\n",
    "\n",
    "As laws relevant to companies outside of the healthcare and financial sectors are still developing, accountability for such entities is currently nebulous. The <u>data lifecycle</u> is a pipeline that considers the past, present, and future of data and its application in technologies by said companies. This pipeline involves numerous steps, such as data collection, processessing, analysis, dissemination, and maintenance, which can be lead by numerous teams or organizations of people. Thus, for terminal applications of data, how do we determine accountability when things go wrong? While assignment of accountability is complex and multifaceted, Virginia Dignum, an AI ethicist and researcher, suggests that when thinking about accountability in the data lifecycle, the larger sociotechnical ecosystem must be considered within a *Accountability-Responsibility-Transparency* (ART) framework [^***********]. Those involved throughout various phases of the data lifecycle should be able to explain and justify their decisions around data (*accountability*), acknowledge the role that they play in the data lifecyle (*responsibility*), and describe, inspect and reproduce mechanisms contributing to end products of the life cycle (*transparency*). Utilizing this framework requires a level of open discussion with stakeholders, whether they be clients, users, or society. Such discussions facilitate iterative modification and reworking to improve products and services from an ethical and technological standpoint.\n",
    "\n",
    "<img align=\"center\" src=\"./img/lifecycleofdata.png\" width=\"50%\"/>\n",
    "\n",
    "<figcaption>The data lifecycle. Adapted from Communications of the ACM, July 2020, Vol. 63 No. 7, Pages 58-66\n",
    "10.1145/3360646</figcaption>\n",
    "\n",
    "\n",
    "\n",
    "## The Trade-Off Between Transparency and Accuracy\n",
    "\n",
    "The development of artificial intelligence algorithms has aided, informed, and/or influenced human decision-making processes in a number of low-stakes and high-stakes scenarios. Because of this, conversation around the transparency of these algorithms has highlighted important perspectives and viewpoints regarding the implications of these technologies. \n",
    "\n",
    "Some algorithms disclose the use of parameters and models applied (i.e., \"white-box\" algorithms), while others may not readily explain the parameters and models used or may utilize another model that approximates the original model(s) (i.e., \"black-box algorithms). Some black-box algorithms also may utilize more sophistocated and complex methods, such as random forests and neural networks, which may not be readily explainable. Depending on the context of use, the methods backing white-box and black-box algorithms can influence the accuracy, and thus impact terminal services, decisions, products, or actions. \n",
    "\n",
    "Explainable AI (XAI) and ML (XML) explores the intersection of transparency and technical applicability in AI and ML. While transparency can help promote accountability and trust, limitations to prioritizing transparency may present in the use of AI/ML. Some of these include malicous and unintended misuse of developed AI/ML tools [^***********], exposure of trade secrets [^************], domain and technical knowledge requirements for full comprehension, and several others. In high-stakes situations, such as the use of AI/ML in healthcare decisions, some argue that accuracy should be more important than transperancy [^*************], while others suggest the use of *interpretable* models instead of black-box algorithms where possible [^**************]. Sometimes, black-box algorithms can be replaced with more simpler and transparent ones with little compromise of accuracy [^***************], [^****************]. In cases where black-box methods must be used, attempts to provide transparency through justification as opposed to explanation may be the best case scenario [^*****************]. An evalulation of the situation and associated risks and rewards, as well as testing of multiple black-box, white-box, and interpretable options, is important in determining the the best way to balance transparency and accuracy. \n",
    "\n",
    "\n",
    "## Honest Statistical Practices\n",
    "\n",
    "A part of getting accurate insights from data includes using honest and appropriate statistical methods during data analysis. Doing such can enhance reproducibility, allowing for economical allocation of time and resources toward follow up studies. Some common pitfalls in research and analysis include <u>h</u>ypothesizing <u>a</u>fter <u>r</u>esults are <u>k</u>nown (HARKing), p-hacking, and cherry-picking. Below is a discussion of each and how they impact research and knowledge generation.\n",
    "\n",
    "#### HARKing\n",
    "As the acroynm states, HARKing is developing a hypothesis about data after knowing the results that the data depict and then reporting conclusions as if they were hypothesized *a priori*. HARKing does not fully disclose the process leading to the hypothesis and conclusions from data and thus can be seen as dishonest in nature. HARKing may or may not include performing statistical analysis and determining significant variables in a dataset; plotting and cross-examining variables can also be a part of HARKing. When performing exploratory studies, examining the relationships between multiple variables from a dataset can be a useful process to generate new hypotheses, but these hypotheses should be tested with a new dataset to confirm previous observations.\n",
    "\n",
    "#### P-hacking\n",
    "\n",
    "P-hacking can involve the use of multiple testing, various kinds of statistical tests, and/or specific subsetting of data in order to generate a significant p-value. Like HARKing, p-hacking does not fully account for the process leading up to a significant result. Because hypothesis testing reports the probability of an observed outcome occuring given that the null hypothesis is true, testing multiple hypothesis on the same data will give a false positive at some point. To address this, multiple testing corrections can be used [^******************].\n",
    "\n",
    "#### Cherry-picking\n",
    "Cherry-picking involves biased selection of data for analysis or reporting conclusions. Cherry-picking can be used to fuel p-hacking or paint an incomplete picture of a research process. Reporting only data that aligns with a hypothesis can be an impediment to those trying to repeat a reported experiment because it can lead researchers down an avoidable rabbit hole. Furthermore, not reporting null data that is not in alignment with a hypothesis can similarly hinder the scientific process. When outliers are found within data, it is only appropriate to exclude it if there is sound justification based on the data collection process or statistical backing.\n",
    "\n",
    "All-in-all, when it comes to data, especially when derived from humans, centering honesty as much as possible is the best policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^*]: Andrade, Chittaranjan. “HARKing, Cherry-Picking, P-Hacking, Fishing Expeditions, and Data Dredging and Mining as Questionable Research Practices.” The Journal of Clinical Psychiatry, vol. 82, no. 1, Feb. 2021, p. 20f13804, https://doi.org/10.4088/JCP.20f13804.\n",
    "\n",
    "[^**]: Lin, D., Crabtree, J., Dillo, I. et al. The TRUST Principles for digital repositories. Sci Data 7, 144 (2020). https://doi.org/10.1038/s41597-020-0486-7\n",
    "\n",
    "[^***]: Atske, Sara. “2. Americans Concerned, Feel Lack of Control over Personal Data Collected by Both Companies and the Government.” Pew Research Center: Internet, Science & Tech, 15 Nov. 2019, https://www.pewresearch.org/internet/2019/11/15/americans-concerned-feel-lack-of-control-over-personal-data-collected-by-both-companies-and-the-government/.\n",
    "\n",
    "[^****]: Robles, Pedro, and Daniel J. Mallinson. “Artificial Intelligence Technology, Public Trust, and Effective Governance.” Review of Policy Research, May 2023, p. ropr.12555, https://doi.org/10.1111/ropr.12555.\n",
    "\n",
    "[^*****]: Mathews, Lee. “Equifax Data Breach Impacts 143 Million Americans.” Forbes, https://www.forbes.com/sites/leemathews/2017/09/07/equifax-data-breach-impacts-143-million-americans/. Accessed 31 July 2023.\n",
    "\n",
    "[^******]: U.S. Department of Homeland Security. Increasing Threat of DEEPFAKE Identities. Accessed 31 July 2023. https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf\n",
    "\n",
    "[^*******]: “Americans Widely Distrust Facebook, TikTok and Instagram with Their Data, Poll Finds.” Washington Post, 22 Dec. 2021, https://www.washingtonpost.com/technology/2021/12/22/tech-trust-survey/.\n",
    "\n",
    "[^********]: Nadeem, Reem. “Public Awareness of Artificial Intelligence in Everyday Activities.” Pew Research Center Science & Society, 15 Feb. 2023, https://www.pewresearch.org/science/2023/02/15/public-awareness-of-artificial-intelligence-in-everyday-activities/.\n",
    "\n",
    "[^**********]: Morey, Timothy, et al. “Customer Data: Designing for Transparency and Trust.” Harvard Business Review, 1 May 2015, https://hbr.org/2015/05/customer-data-designing-for-transparency-and-trust.\n",
    "\n",
    "[^***********]: Virginia Dignum. The role and challenges of education for responsible AI. London Review of Education. 2021. Vol. 19(1). DOI: 10.14324/LRE.19.1.01\n",
    "\n",
    "[^***********]: Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura, and Peter Eckersley. 2020. Explainable machine learning in deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 648–657. https://doi.org/10.1145/3351095.3375624\n",
    "\n",
    "[^************]: Burrell, J. (2016). How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data & Society, 3(1). https://doi.org/10.1177/2053951715622512\n",
    "\n",
    "[^*************]: Ghassemi M, Oakden-Rayner L, Beam AL. The false hope of current approaches to explainable artificial intelligence in health care. Lancet Digit Health. 2021 Nov;3(11):e745-e750. doi: 10.1016/S2589-7500(21)00208-9. PMID: 34711379.\n",
    "\n",
    "[^**************]: Rudin C. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. Nat Mach Intell. 2019 May;1(5):206-215. doi: 10.1038/s42256-019-0048-x. Epub 2019 May 13. PMID: 35603010; PMCID: PMC9122117.\n",
    "\n",
    "[^***************]: Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. 2019. This looks like that: deep learning for interpretable image recognition. Proceedings of the 33rd International Conference on Neural Information Processing Systems. Curran Associates Inc., Red Hook, NY, USA, Article 801, 8930–8941.\n",
    "\n",
    "[^****************]: Barnett, A.J., Schwartz, F.R., Tao, C. et al. A case-based interpretable deep learning model for classification of mass lesions in digital mammography. Nat Mach Intell 3, 1061–1070 (2021). https://doi.org/10.1038/s42256-021-00423-x\n",
    "\n",
    "[^*****************]: Biran, Or and Courtenay V. Cotton. “Explanation and Justification in Machine Learning : A Survey Or.” (2017).\n",
    "\n",
    "[^******************]: Andrade C. Multiple Testing and Protection Against a Type 1 (False Positive) Error Using the Bonferroni and Hochberg Corrections. Indian Journal of Psychological Medicine. 2019;41(1):99-100. doi:10.4103/IJPSYM.IJPSYM_499_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
