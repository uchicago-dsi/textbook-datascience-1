## Cherry-picking
Selecting only the data, examples, or results that support a desired conclusion, while ignoring evidence that contradicts it. This creates a misleading picture of reality.  

Example: reporting only one experiment where a drug worked, while leaving out several where it did not.

## p-hacking
Manipulating data analysis until statistically significant results (p < 0.05) appear, even if those results are due to chance. This can involve trying many different tests, stopping data collection at just the right point, or selectively reporting results.  

Example: running 20 different tests and only publishing the one that happened to be significant.

## HARKing
"Hypothesizing After the Results are Known."  
Presenting a hypothesis as if it was decided *before* the data analysis, when in fact it was created *after seeing the results*. This undermines the credibility of research, since hypotheses should guide analysis, not be invented from it.  

Example: noticing a surprising relationship in the data and then writing the paper as if that relationship was the original hypothesis.
