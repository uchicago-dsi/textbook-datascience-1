{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Datasets in machine learning can broadly be categorized as structured and unstructured, depending on its format of presentation:\n",
    "- **Structured data**: Structured data is typically stored in tabular formats with a well-defined schema that every single row or datapoint adheres to when being recorded. Structured data can include both quantitative measurements (prices, height, etc.) as well as qualitative records (dates, names, addresses, etc.) This kind of data works well with most machine learning algorithms, can be easy to maintain and interpret.\n",
    "\n",
    "    Example of structured data : Below is a dataset that we have previously used for data analysis. It contains information on different houses in Athens, Ohio. Every record indicates the relationship between the various features of a residence listed in this data and the price at which it was sold in the market. The data is structured because every record is represented using the same set of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>floor_size</th>\n",
       "      <th>bed_room_count</th>\n",
       "      <th>built_year</th>\n",
       "      <th>sold_date</th>\n",
       "      <th>sold_price</th>\n",
       "      <th>room_count</th>\n",
       "      <th>garage_size</th>\n",
       "      <th>parking_lot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2068</td>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>Aug2015</td>\n",
       "      <td>195500</td>\n",
       "      <td>6</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3372</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Dec2015</td>\n",
       "      <td>385000</td>\n",
       "      <td>6</td>\n",
       "      <td>480</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3130</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan2017</td>\n",
       "      <td>188000</td>\n",
       "      <td>7</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3991</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Nov2014</td>\n",
       "      <td>375000</td>\n",
       "      <td>8</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan2015</td>\n",
       "      <td>136000</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   floor_size  bed_room_count  built_year sold_date  sold_price  room_count  \\\n",
       "0        2068               3        2003   Aug2015      195500           6   \n",
       "1        3372               3        1999   Dec2015      385000           6   \n",
       "2        3130               3        1999   Jan2017      188000           7   \n",
       "3        3991               3        1999   Nov2014      375000           8   \n",
       "4        1450               2        1999   Jan2015      136000           7   \n",
       "\n",
       "   garage_size  parking_lot  \n",
       "0          768            3  \n",
       "1          480            2  \n",
       "2          400            2  \n",
       "3          400            2  \n",
       "4          200            1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @hidden_cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "housing_df = pd.read_csv(\"../../data/Housing.csv\")\n",
    "housing_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Unstructured data**: Unstructured data does not have a standard, pre-defined format and typically resembles data in its raw form as closely as possible. Data consisting of texts, images, videos or multimedia files typically fall under this category.\n",
    "\n",
    "    Example of unstructured data: Here is a snapshot of the popular IMDB dataset often used to train models that consume text snippets to predict the sentiment expressed in them. The dataset contains 50K reviews posted by users of the website with a positive or negative sentiment label. Unlike the housing data shown above, the input is not represented through a fixed set of features and instead the text itself is the only feature available.\n",
    "\n",
    "    <img src=\"imdb.png\" alt=\"IMDB\" />\n",
    "\n",
    "Feature engineering strategies vary depending on the type and format of data to which they are being applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features in Machine Learning\n",
    "\n",
    "Most machine learning tasks start with data retrieval that gathers raw data to be ingested into a model. This is followed by a data preparation process where different techniques are tried to engineer meaningful features that the model of choice can utilize during training. The trained model is then deployed for the subsequent prediction (regression or classification) task on unseen data. Note that data used for testing also undergo similar transformations, made earlier on the training data, prior to being fed to the model to generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Techniques in Structured Data\n",
    "\n",
    "Structured data is standardized, clearly defined in format, as well as easy to organize, search and analyze. As mentioned before, data types stored in structured data can be both numeric or categorical. We will look into specific feature engineering techniques for each of these data types now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering on Numeric Data\n",
    "\n",
    "Numeric or quantitative data consist of scalar values that record measurements or observations, often in certain prespecified units. You can learn more about this type of data in [Section 9.2](../../9/2/Numerical_Data.ipynb).\n",
    "\n",
    "Raw numeric data can be fed directly into most models but depending on the problem and application domain they could still be modified to better features. In this section, we will look into a few strategies we can leverage for feature engineering on numeric data. We will use two datasets at our disposal to demonstrate these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin our review of feature engineering techniques by applying them on the previously used diabetes dataset. \n",
    "\n",
    "The subjects of this dataset are pregnant people aged 21 years or older. A record for each patient in this dataset consists of a number of biographical and health markers with the goal of being able to predict if they are diabetic or not. The latter information is stored in a binary variable called 'Outcome' which is the response variable in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df = pd.read_csv(\"../../data/diabetes.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scaling/Normalization**: \n",
    "Feature Scaling is an important problem to tackle when feeding numeric data to models. In order to train a model and enhance its predictive capacity features should preferably be within a similar range and not vastly disparate scales. Min-max normalization is a common way of feature scaling where all values are scaled down to a range between [0, 1]. The resulting transformation has no influence on the feature's underlying distribution but could be sensitive to the presence of outliers that could affect the minimum and maximum feature values and as a result the underlying scale. To put this in a formula, scaling for feature $x$ is conducted as follows:\n",
    "\n",
    "    > $x'=\\frac{x-min(x)}{max(x)-min(x)}$\n",
    "\n",
    "    Here in the diabetes dataset, we look to normalize the feature column *Glucose* and bring down the range of values to fit within [0,1]. Note that we should ideally repeat this process with all numeric features in the dataset to bring them down to the same scale prior to model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Glucose_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  Glucose_normalized  \n",
       "0                     0.627   50        1            0.743719  \n",
       "1                     0.351   31        0            0.427136  \n",
       "2                     0.672   32        1            0.919598  \n",
       "3                     0.167   21        0            0.447236  \n",
       "4                     2.288   33        1            0.688442  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column = 'Glucose'\n",
    "diabetes_df['Glucose_normalized'] = MinMaxScaler().fit_transform(np.array(diabetes_df[column]).reshape(-1,1))\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Standardization**:\n",
    "Another approach to normalize data is Standardization or z-score normalization, that takes into account the underlying variance of the feature distribution. To standardize a feature column, all data points are subtracted by their mean value and the result divided by the feature distribution's standard deviation. The transformed data points represent z-scores of the initial feature values. Recall that we have explored this concept previously in [Section 17.2](../../17/2/correlation.ipynb). To put this in a formula, the standardization of feature $x$ with mean value $\\mu$ and standard deviation $\\sigma$ is conducted as follows:\n",
    "    > $x'=\\frac{x-\\mu}{\\sigma}$\n",
    "\n",
    "    With this transformation, we arrive at a feature distribution of 0 mean and unit variance. Since the standardization process does not limit the transformed values within a specific range, the outliers within data do not impact the transformation process. However it does enforce the assumption that the feature is normally distributed which may not always be true.\n",
    "\n",
    "    In the following code, we apply z-score normalization to the *BMI* feature. Although we have previously implemented a function `standard_units` in [Section 17.2](../../17/2/correlation.ipynb) to convert feature values to their corresponding z-scores, we can alternatively use the `StandardScaler` to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Glucose_normalized</th>\n",
       "      <th>BMI_standardized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.204013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>-0.684422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>-1.103255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>-0.494043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>1.409746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  Glucose_normalized  \\\n",
       "0                     0.627   50        1            0.743719   \n",
       "1                     0.351   31        0            0.427136   \n",
       "2                     0.672   32        1            0.919598   \n",
       "3                     0.167   21        0            0.447236   \n",
       "4                     2.288   33        1            0.688442   \n",
       "\n",
       "   BMI_standardized  \n",
       "0          0.204013  \n",
       "1         -0.684422  \n",
       "2         -1.103255  \n",
       "3         -0.494043  \n",
       "4          1.409746  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column = 'BMI'\n",
    "diabetes_df['BMI_standardized'] = StandardScaler().fit_transform(np.array(diabetes_df[column]).reshape(-1,1))\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} About Normalization and Standardization in practice\n",
    ":class: tip\n",
    "A common mistake encountered in practice is applying these techniques separately on training and test data. Instead, we first split our data into training and test sets and apply `MinMaxScaler().fit_transform` or `StandardScaler().fit_transform` on the training set and the corresponding `transform` function on the test set. This is done to obtain the appropriate scale in which feature values are to be normalized, or in the case of standardization, the mean and variance of the feature distribution from the training split.\n",
    "\n",
    "This is aligned with an important principle of machine learning that training data is the only information that is available and should be used to learn model parameters. Or else, fitting the `MinMaxScaler` or `StandardScaler` on the test data leads to data leakage where information from the test set inadverdently influences the training process by projecting over-optimistic performance estimates.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Binarization**: \n",
    "Often features represent raw counts or frequencies whose exact values are less relevant to the problem at hand. Instead, the prediction task might only need to rely on the feature for being indicative of a certain phenomenon in the data space. In such cases, binarization of a numeric feature can resolve the scaling issue, that we have navigated using previous techniques, by transforming the original feature to an indicator function. This also simplifies the learning problem by significantly reducing the range of values that the underlying model has to deal with during training.\n",
    "\n",
    "    In the following code, we apply binarization to the *Pregnancies* column, which indiciates the number of pregnanices that each subject in the dataset has experienced, and simplify the information in a binary feature 'was_pregnant' to indicate whether a subject has prior experience of carrying a child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Glucose_normalized</th>\n",
       "      <th>BMI_standardized</th>\n",
       "      <th>was_pregnant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>-0.684422</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>-1.103255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>-0.494043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>1.409746</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  Glucose_normalized  \\\n",
       "0                     0.627   50        1            0.743719   \n",
       "1                     0.351   31        0            0.427136   \n",
       "2                     0.672   32        1            0.919598   \n",
       "3                     0.167   21        0            0.447236   \n",
       "4                     2.288   33        1            0.688442   \n",
       "\n",
       "   BMI_standardized  was_pregnant  \n",
       "0          0.204013             1  \n",
       "1         -0.684422             1  \n",
       "2         -1.103255             1  \n",
       "3         -0.494043             1  \n",
       "4          1.409746             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column = 'Pregnancies'\n",
    "was_pregnant = np.array(diabetes_df[column])\n",
    "was_pregnant[was_pregnant>=1] = 1\n",
    "diabetes_df['was_pregnant'] = was_pregnant\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Rounding**: \n",
    "Often when dealing with continuous numeric attributes the model might not require scalar values to be maintained at a high precision. In such cases, it makes sense to round off high precision floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Glucose_normalized</th>\n",
       "      <th>BMI_standardized</th>\n",
       "      <th>was_pregnant</th>\n",
       "      <th>rounded_DiabetesPedigreeFunction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>-0.684422</td>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>-1.103255</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>-0.494043</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>1.409746</td>\n",
       "      <td>0</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  Glucose_normalized  \\\n",
       "0                     0.627   50        1            0.743719   \n",
       "1                     0.351   31        0            0.427136   \n",
       "2                     0.672   32        1            0.919598   \n",
       "3                     0.167   21        0            0.447236   \n",
       "4                     2.288   33        1            0.688442   \n",
       "\n",
       "   BMI_standardized  was_pregnant  rounded_DiabetesPedigreeFunction  \n",
       "0          0.204013             1                              0.63  \n",
       "1         -0.684422             1                              0.35  \n",
       "2         -1.103255             1                              0.67  \n",
       "3         -0.494043             1                              0.17  \n",
       "4          1.409746             0                              2.29  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df['rounded_DiabetesPedigreeFunction'] = diabetes_df['DiabetesPedigreeFunction'].round(2)\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Custom Features**: \n",
    "Having domain knowledge can often help aggregate multiple raw features into new custom features that can better capture context more directly relevant to the predictive task at hand.\n",
    "\n",
    "    For example, let us take another look at the following dataset that contains information of housing prices in Athens, Ohio. Each listing is described through a number of features including floor area, garage area and its corresponding price. We can simplify this information into a single custom feature that stores the price of a listing per unit area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>floor_size</th>\n",
       "      <th>bed_room_count</th>\n",
       "      <th>built_year</th>\n",
       "      <th>sold_date</th>\n",
       "      <th>sold_price</th>\n",
       "      <th>room_count</th>\n",
       "      <th>garage_size</th>\n",
       "      <th>parking_lot</th>\n",
       "      <th>total_size</th>\n",
       "      <th>price_per_area_unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2068</td>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>Aug2015</td>\n",
       "      <td>195500</td>\n",
       "      <td>6</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>2836</td>\n",
       "      <td>68.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3372</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Dec2015</td>\n",
       "      <td>385000</td>\n",
       "      <td>6</td>\n",
       "      <td>480</td>\n",
       "      <td>2</td>\n",
       "      <td>3852</td>\n",
       "      <td>99.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3130</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan2017</td>\n",
       "      <td>188000</td>\n",
       "      <td>7</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>3530</td>\n",
       "      <td>53.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3991</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Nov2014</td>\n",
       "      <td>375000</td>\n",
       "      <td>8</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>4391</td>\n",
       "      <td>85.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan2015</td>\n",
       "      <td>136000</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1650</td>\n",
       "      <td>82.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   floor_size  bed_room_count  built_year sold_date  sold_price  room_count  \\\n",
       "0        2068               3        2003   Aug2015      195500           6   \n",
       "1        3372               3        1999   Dec2015      385000           6   \n",
       "2        3130               3        1999   Jan2017      188000           7   \n",
       "3        3991               3        1999   Nov2014      375000           8   \n",
       "4        1450               2        1999   Jan2015      136000           7   \n",
       "\n",
       "   garage_size  parking_lot  total_size  price_per_area_unit  \n",
       "0          768            3        2836                68.94  \n",
       "1          480            2        3852                99.95  \n",
       "2          400            2        3530                53.26  \n",
       "3          400            2        4391                85.40  \n",
       "4          200            1        1650                82.42  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df = pd.read_csv(\"../../data/Housing.csv\")\n",
    "housing_df['total_size'] = housing_df['floor_size']+housing_df['garage_size']\n",
    "housing_df['price_per_area_unit'] = (housing_df['sold_price']/housing_df['total_size']).round(2)\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Polynomial Transformations**: \n",
    "Polynomial expansions of continuous valued features are common transformations to achieve higher order features that can be linearly combined in the eventual optimization function. For example, in case of a continuous predictor *x* , an order *p* polynomial expansion would yield the following additional features:\\\n",
    "    f(x) = $\\sum_{i=1}^p \\beta_{i}x^{i}$, where p is a hyperparameter that can be selected during finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Trigonometric Transformations**: \n",
    "Sometimes features found in datasets can be cyclical in nature. Timeseries, wind or tidal data typically constitute of cyclical variables where values repeat periodically. It is vital for such features to be transformed into a representation where the model can exploit their cyclical nature to improve its predictive capability. In such cases trigonometric transformations are commonly used. A feature variable $t$ can be converted into a set of cyclical features:\\\n",
    "    x = $\\sin(\\frac{t\\times2\\pi}{max(t)})$, and, y = $\\cos(\\frac{t\\times2\\pi}{max(t)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Logarithmic Transformations**: \n",
    "Log transforms are applied to features with skewed distributions in order to control the skewness. We take the log of the values in the feature column to bring down its range and feed the transformed feature to the model. However logarithmic transformations do not work on features with non-positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550000 87000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>floor_size</th>\n",
       "      <th>bed_room_count</th>\n",
       "      <th>built_year</th>\n",
       "      <th>sold_date</th>\n",
       "      <th>sold_price</th>\n",
       "      <th>room_count</th>\n",
       "      <th>garage_size</th>\n",
       "      <th>parking_lot</th>\n",
       "      <th>total_size</th>\n",
       "      <th>price_per_area_unit</th>\n",
       "      <th>sold_price_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2068</td>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>Aug2015</td>\n",
       "      <td>195500</td>\n",
       "      <td>6</td>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>2836</td>\n",
       "      <td>68.94</td>\n",
       "      <td>12.183316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3372</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Dec2015</td>\n",
       "      <td>385000</td>\n",
       "      <td>6</td>\n",
       "      <td>480</td>\n",
       "      <td>2</td>\n",
       "      <td>3852</td>\n",
       "      <td>99.95</td>\n",
       "      <td>12.860999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3130</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan2017</td>\n",
       "      <td>188000</td>\n",
       "      <td>7</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>3530</td>\n",
       "      <td>53.26</td>\n",
       "      <td>12.144197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3991</td>\n",
       "      <td>3</td>\n",
       "      <td>1999</td>\n",
       "      <td>Nov2014</td>\n",
       "      <td>375000</td>\n",
       "      <td>8</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>4391</td>\n",
       "      <td>85.40</td>\n",
       "      <td>12.834681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>Jan2015</td>\n",
       "      <td>136000</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1650</td>\n",
       "      <td>82.42</td>\n",
       "      <td>11.820410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   floor_size  bed_room_count  built_year sold_date  sold_price  room_count  \\\n",
       "0        2068               3        2003   Aug2015      195500           6   \n",
       "1        3372               3        1999   Dec2015      385000           6   \n",
       "2        3130               3        1999   Jan2017      188000           7   \n",
       "3        3991               3        1999   Nov2014      375000           8   \n",
       "4        1450               2        1999   Jan2015      136000           7   \n",
       "\n",
       "   garage_size  parking_lot  total_size  price_per_area_unit  sold_price_log  \n",
       "0          768            3        2836                68.94       12.183316  \n",
       "1          480            2        3852                99.95       12.860999  \n",
       "2          400            2        3530                53.26       12.144197  \n",
       "3          400            2        4391                85.40       12.834681  \n",
       "4          200            1        1650                82.42       11.820410  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(housing_df['sold_price'].max(), housing_df['sold_price'].min())\n",
    "housing_df['sold_price_log'] = np.log(housing_df['sold_price'])\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering on Categorical Data\n",
    "\n",
    "Categorical predictors are those that contain qualitative data. For example, education level, state of residence, or even zipcode (which albeit have numerical values) would qualify as categorical data. You can learn more about this type of data in [Section 9.3](../../9/3/Categorical_Data.ipynb).\n",
    "\n",
    "Categorical variables can have both ordered or unordered data depending on whether the data values can be organized based on some inherent ordering among them. If we look into this fictional student scores dataset with records of math, reading and writing test scores for every student, the feature *'parental level of education'* shows a clear ordering among its categorical values. Hence this feature consists of *ordinal data*. On the other hand, *'gender'* is a categorical feature with values that do not have any natural ordering within them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>group D</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>57</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>57</td>\n",
       "      <td>69</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>74</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender race/ethnicity parental level of education         lunch  \\\n",
       "0   female        group D                some college      standard   \n",
       "1     male        group D          associate's degree      standard   \n",
       "2   female        group D                some college  free/reduced   \n",
       "3     male        group B                some college  free/reduced   \n",
       "4   female        group D          associate's degree      standard   \n",
       "5     male        group C            some high school      standard   \n",
       "6   female        group E          associate's degree      standard   \n",
       "7   female        group B            some high school      standard   \n",
       "8     male        group C            some high school      standard   \n",
       "9   female        group C           bachelor's degree      standard   \n",
       "10    male        group B            some high school      standard   \n",
       "11    male        group B             master's degree      standard   \n",
       "12    male        group B           bachelor's degree  free/reduced   \n",
       "13    male        group A                some college      standard   \n",
       "14    male        group C             master's degree  free/reduced   \n",
       "\n",
       "   test preparation course  math score  reading score  writing score  \n",
       "0                completed          59             70             78  \n",
       "1                     none          96             93             87  \n",
       "2                     none          57             76             77  \n",
       "3                     none          70             70             63  \n",
       "4                     none          83             85             86  \n",
       "5                     none          68             57             54  \n",
       "6                     none          82             83             80  \n",
       "7                     none          46             61             58  \n",
       "8                     none          80             75             73  \n",
       "9                completed          57             69             77  \n",
       "10                    none          74             69             69  \n",
       "11                    none          53             50             49  \n",
       "12                    none          76             74             76  \n",
       "13                    none          70             73             70  \n",
       "14                    none          55             54             52  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_scores_df = pd.read_csv(\"../../data/student_scores_data.csv\")\n",
    "student_scores_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordered and unordered features require different preprocessing approaches for the underlying information to be fed into a model. Although tree-based models (to be covered in [Chapter 25](../../25/placeholder25.ipynb)) are capable of handling raw categorical data, majority of models require numeric predictors as input. Hence, in this section, we will look into a few strategies we can utilize to engineer model-friendly features from categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **One-hot Encoding**: \n",
    "The simplest way to handle categorical data is to create a vector of **indicator variables**, one for each category. These are variables artificially added to the feature set to capture the presence of different possible values for a categorical feature. To illustrate this consider the categorical feature *'race/ethnicity'* in the student scores dataset. We look into the possible values and convert them into dummy binary variables. It is also acceptable to create these dummy variables for all but one of the values. The reason to leave one of the values out is that it can be directly inferred from the states of the other variables. Including dummy variables for every single value a categotical feature takes could therefore add multicollinearity. In the following code, we use the `get_dummies` function from the Pandas library to accomplish this. The newly created variables are appended to the dataset thereby increasing the number of columns.  \n",
    "Even though this encoding strategy increases the dimensionality of data at hand, it does not impose any ordering that does not exist among categories unlike some of the other techniques that we will examine later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group A', 'group B', 'group C', 'group D', 'group E'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(student_scores_df['race/ethnicity'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>race/ethnicity_group B</th>\n",
       "      <th>race/ethnicity_group C</th>\n",
       "      <th>race/ethnicity_group D</th>\n",
       "      <th>race/ethnicity_group E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>87</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>57</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>58</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>male</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>57</td>\n",
       "      <td>69</td>\n",
       "      <td>77</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>male</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>74</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>male</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>male</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>male</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>male</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender parental level of education         lunch test preparation course  \\\n",
       "0   female                some college      standard               completed   \n",
       "1     male          associate's degree      standard                    none   \n",
       "2   female                some college  free/reduced                    none   \n",
       "3     male                some college  free/reduced                    none   \n",
       "4   female          associate's degree      standard                    none   \n",
       "5     male            some high school      standard                    none   \n",
       "6   female          associate's degree      standard                    none   \n",
       "7   female            some high school      standard                    none   \n",
       "8     male            some high school      standard                    none   \n",
       "9   female           bachelor's degree      standard               completed   \n",
       "10    male            some high school      standard                    none   \n",
       "11    male             master's degree      standard                    none   \n",
       "12    male           bachelor's degree  free/reduced                    none   \n",
       "13    male                some college      standard                    none   \n",
       "14    male             master's degree  free/reduced                    none   \n",
       "\n",
       "    math score  reading score  writing score  race/ethnicity_group B  \\\n",
       "0           59             70             78                   False   \n",
       "1           96             93             87                   False   \n",
       "2           57             76             77                   False   \n",
       "3           70             70             63                    True   \n",
       "4           83             85             86                   False   \n",
       "5           68             57             54                   False   \n",
       "6           82             83             80                   False   \n",
       "7           46             61             58                    True   \n",
       "8           80             75             73                   False   \n",
       "9           57             69             77                   False   \n",
       "10          74             69             69                    True   \n",
       "11          53             50             49                    True   \n",
       "12          76             74             76                    True   \n",
       "13          70             73             70                   False   \n",
       "14          55             54             52                   False   \n",
       "\n",
       "    race/ethnicity_group C  race/ethnicity_group D  race/ethnicity_group E  \n",
       "0                    False                    True                   False  \n",
       "1                    False                    True                   False  \n",
       "2                    False                    True                   False  \n",
       "3                    False                   False                   False  \n",
       "4                    False                    True                   False  \n",
       "5                     True                   False                   False  \n",
       "6                    False                   False                    True  \n",
       "7                    False                   False                   False  \n",
       "8                     True                   False                   False  \n",
       "9                     True                   False                   False  \n",
       "10                   False                   False                   False  \n",
       "11                   False                   False                   False  \n",
       "12                   False                   False                   False  \n",
       "13                   False                   False                   False  \n",
       "14                    True                   False                   False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_categorical_feature(original_dataframe, feature_to_encode):\n",
    "    #function to generate one-hot encoded features from categorical values\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]], drop_first=True)\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return res\n",
    "\n",
    "feature = 'race/ethnicity'\n",
    "encoded_df = encode_categorical_feature(student_scores_df, feature)\n",
    "encoded_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A drawback of the one-hot encoding setup is when the set of possible values for a categorical feature gets too large. For example encoding a categorical feature like ZIP code for United States could consist of up to 41K values. Applying the one-hot encoding strategy would lead to an overabundance of dummy variables relative to the number of datapoints available for effective model training. Moreover, due to uneven distribution of population across different locations, one might encounter certain zip codes much more frequently than others, leading to a long tailed feature distribution when collecting data.\n",
    "\n",
    "An issue with having such long-tailed feature distribution is that resampling the data might altogether exclude some infrequent categories from the analysis altogether. This could lead to dummy variable columns of all zeros and result in a numerical error for many models rendering them incapable of producing accurate predictions for test samples that do contain these categories. Feature columns with a single value are called zero-variance predictors that do not provide meaningful representation for the predictive task at hand. While we can create the full set of indicator variables and filter out those showing near-zero variance, the latter cannot be known a priori. As an alternative, these predictors can be pooled together to an \"Other\" category. Another way to combine categories would be to use a hash function and group categories into a reduced set of hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Label Encoding**: \n",
    "Alternative to one-hot encoding, label encoding does not add any additional feature columns to the data and maps each unique category to a number. Such a numerical mapping however adds an ordering among the transformed values which might not exist among the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ordinal Encoding**: \n",
    "However, ordered categorical values exist. For example, `parental level of education' has categories that can be ordered by the degree of education that students' parents that completed. When categories have a natural ordering among them, a numerical mapping of categories to values that preserves the same ordering makes sense and would also improve the underlying predictive task. Hence, such an ordering is called an Ordinal Encoding. Like label encoding, the data dimensionality is not increased during such transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Although both Label and Ordinal Encoding tranform categorical feature values to numerical ones, you use label encoding when the feature values do not have any order amongst themselves. For example, in the students score dataset, a feature like race would be appropriate for Label Encoding. Conversely, Ordinal Encoding is used while being mindful of the order in the data. For example, a feature like temperature with values 'hot', 'warm', 'cold' will be more appropriate for Ordinal Encoding.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>parental_education_levels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>group D</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>87</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>57</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>57</td>\n",
       "      <td>69</td>\n",
       "      <td>77</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>some high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>74</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "      <td>70</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>52</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender race/ethnicity parental level of education         lunch  \\\n",
       "0   female        group D                some college      standard   \n",
       "1     male        group D          associate's degree      standard   \n",
       "2   female        group D                some college  free/reduced   \n",
       "3     male        group B                some college  free/reduced   \n",
       "4   female        group D          associate's degree      standard   \n",
       "5     male        group C            some high school      standard   \n",
       "6   female        group E          associate's degree      standard   \n",
       "7   female        group B            some high school      standard   \n",
       "8     male        group C            some high school      standard   \n",
       "9   female        group C           bachelor's degree      standard   \n",
       "10    male        group B            some high school      standard   \n",
       "11    male        group B             master's degree      standard   \n",
       "12    male        group B           bachelor's degree  free/reduced   \n",
       "13    male        group A                some college      standard   \n",
       "14    male        group C             master's degree  free/reduced   \n",
       "\n",
       "   test preparation course  math score  reading score  writing score  \\\n",
       "0                completed          59             70             78   \n",
       "1                     none          96             93             87   \n",
       "2                     none          57             76             77   \n",
       "3                     none          70             70             63   \n",
       "4                     none          83             85             86   \n",
       "5                     none          68             57             54   \n",
       "6                     none          82             83             80   \n",
       "7                     none          46             61             58   \n",
       "8                     none          80             75             73   \n",
       "9                completed          57             69             77   \n",
       "10                    none          74             69             69   \n",
       "11                    none          53             50             49   \n",
       "12                    none          76             74             76   \n",
       "13                    none          70             73             70   \n",
       "14                    none          55             54             52   \n",
       "\n",
       "    parental_education_levels  \n",
       "0                         2.0  \n",
       "1                         3.0  \n",
       "2                         2.0  \n",
       "3                         2.0  \n",
       "4                         3.0  \n",
       "5                         0.0  \n",
       "6                         3.0  \n",
       "7                         0.0  \n",
       "8                         0.0  \n",
       "9                         4.0  \n",
       "10                        0.0  \n",
       "11                        5.0  \n",
       "12                        4.0  \n",
       "13                        2.0  \n",
       "14                        5.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parental_education_levels = set(student_scores_df[\"parental level of education\"])\n",
    "parental_education_levels_categories = ['some high school','high school','some college',\"associate's degree\",\"bachelor's degree\",\"master's degree\"]\n",
    "encoder = OrdinalEncoder(categories=[parental_education_levels_categories])\n",
    "student_scores_df['parental_education_levels'] = encoder.fit_transform(student_scores_df[[\"parental level of education\"]])\n",
    "student_scores_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering on Unstructured Text Data\n",
    "\n",
    "Data practitioners often have to deal with data containing textual fields or unstructured text data for certain learning tasks. Data containing textual fields can be gathered from questionnaires, reviews, tweets or large-scale collection of documents otherwise called **corpus** (for example, collection of Shakespearean sonnets). For these datasets, words or phrases (sequence of consecutive words known as *n-grams*) populating the open text fields act as predictors for the machine learning task at hand. Hence, we need to find a process that transforms their absence or presence into a numerical representation of such textual data. This technique is referred to as **Text Vectorization**. Prior to this, data practitioners conduct a handful of text pre-processing and cleaning steps. These consist of:\n",
    "- Text Normalization: Case folding along with removal of punctuations or special characters\n",
    "- Tokenization: Segregating textual data into individual tokens, i.e, surface forms in which they appear in the text. \n",
    "- Stemming or lemmatization: Transforming tokens obtained in the previous step, i.e. the full inflected forms in which words appear in text, to their root forms, often by dropping suffixes. For example, the token *'jumping'* will be converted to the root word *'jump'*.\n",
    "- Removal of stopwords: Stopwords are functional words (like common prepositions or conjunctions) that appear frequently in text across all contexts and are therefore not considered to be discriminative features. Including these stopwords in text analysis brings in noise and skews the frequency distributions associated with words or tokens in your text data. Hence, it is important to remove them.\n",
    " \n",
    "Text Vectorization begins with setting a vocabulary, $V$, that comprises of all the possible distinct words encountered in a text corpus. Next we explore strategies that convert text data into $|V|$ dimensional vectors of binary or real-valued features. To understand these better, let us start with a simple example of a mini document collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:example}\n",
    ":label: my-text-example\n",
    "\n",
    "A corpus of 3 documents:\n",
    "\n",
    "- $D_{1}$: Hello darkness my old friend.\n",
    "- $D_{2}$: Ignorance is a manner of darkness.\n",
    "- $D_{3}$: He leapt into the darkness of night.\n",
    "\n",
    "After some initial cleanup and text pre-processing steps that also includes removing stopwords (*is*, *a*, *of*, *into*, etc.), we can see that this mini-corpus has a vocabulary of 10 unique words.\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'night', 'leapt', 'ignorance', 'manner', 'friend', 'hello', 'old', 'darkness'}\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = [\"Hello darkness my old friend.\",\n",
    "          \"Ignorance is a manner of darkness.\",\n",
    "          \"He leapt into the darkness of night.\"\n",
    "        ]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "vocabulary = set()\n",
    "for document in corpus:\n",
    "    word_tokens = word_tokenize(document)\n",
    "    for word in word_tokens:\n",
    "        if (word.lower() not in stop_words) & (word.lower() not in punctuations):\n",
    "            vocabulary.add(word.lower())\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding\n",
    "\n",
    "The simplest vectorization technique is to treat the words in the corpus vocabulary as categorical features and to associate an indicator variable with each word in the feature vector. However, one-hot encoding can only signify the presence or absence of certain words in text. In many text applications, frequency of words play an important role in measuring their relative importance within the corpus, as well as, to the predictive task at hand, which is why we often prefer alternative strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Bag of Words representation\n",
    "\n",
    "This is a popular representation of text data frequently utilized in the field of Information Retrieval. In bag-of-words models the input text is converted into a $|V|$ dimensional real-valued vector of word counts or frequencies. Bag-of-word (BOW) representation converts a text document into a flat vector. While we can encode the relative importance of words within a corpus through frequency features, this representation treat text data as an unordered collection of tokens. Since the ordering of words in text indicate both meaning and context, bag-of-words representation cannot encode any semantic information.\n",
    "\n",
    "#### TF-IDF model\n",
    "\n",
    "This method is an improvement over the previously described BOW model that simply records word counts in feature vectors. The TF-IDF statistic considers two different kinds of frequencies:\n",
    "\n",
    "1. Term Frequency (tf) - For a word $w$ and document $D$ in the corpus, $tf(w,D)$ represents the frequency or raw count of the word in the document.\n",
    "2. Inverse Document Frequency (idf) - This frequency is a signifier of the informativeness of a term in the context of the whole corpus. It assumes that much like stopwords, if a word in a corpus appears widely in most or all document, it's informativeness relative to the content of individual documents is diminished. Rare words are considered more interesting since they can provide distinctive information. Hence, $idf$ applies a log transform upon the inverse of a word's document frequency. If in a corpus of $N$ documents, the word $w$ appears in $df(w)$ documents, then $idf(w,D) = log(\\frac{N}{df(w)})$.\n",
    "\n",
    "The combined $tfidf$ statistic is calculated as the product of the above two frequencies:\\\n",
    "    $tfidf(w,D) = tf(w,D)\\times idf(w,D)$\n",
    "\n",
    "Feature vectors in the TF-IDF model represents each document in the corpus by including the $tfidf$ score of every word in the vocabulary corresponding to the document. These scores are normalized to values between 0 and 1 and the resulting document vectors can be directly fed into a learning algorithm for the downstream prediction task.\n",
    "\n",
    "#### N-gram representation\n",
    "\n",
    "As mentioned earlier, treating texts as unordered collection of words only results in lexical features that do not capture meaning or context. For example, consider the following two sentences:\n",
    "- *The cat killed curiosity.*\n",
    "- *Curiosity killed the cat.*\n",
    "\n",
    "These two sentences carry the opposite sense however they will have the exact same bag-of-words representation. \n",
    "A modification of the Bag-of-word (BOW) representation that addresses this deficit is the $n$-gram model. Individual words are called unigram however a sequence of $n$ consecutive words within a document is called an $n$-gram. Instead of creating a vocabulary of unigrams, this representation creates a vocabulary of all distinct $n$-grams within the corpus, and then, recalculates the previous TF-IDF statistics of $n$-grams for documents within the corpus. Unlike unigrams, $n$-grams obviously retain the ordering of words in these phrases and therefore is a better representation to capture semantic information within text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
